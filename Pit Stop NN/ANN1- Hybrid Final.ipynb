{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import class_weight\n",
    "from keras.callbacks import History\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.layers import LSTM, Dropout, SimpleRNN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb0b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('f1dataset1.csv', encoding='utf-8')\n",
    "\n",
    "# HANDLING NANs\n",
    "df['laptime'] = np.where(df['laptime'].isna(), 0.0, df['laptime'])\n",
    "df['race_progress'] = np.where(df['race_progress'].isna(), 0.0, df['race_progress'])\n",
    "df['tyreageprogress'] = np.where(df['tyreageprogress'].isna(), 0.0, df['tyreageprogress'])\n",
    "\n",
    "# shuffle data\n",
    "shuffled_data = df.sample(frac=1, random_state=42)  # Set random_state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f07a6bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, Reshape, SimpleRNN, Flatten, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Step 1 - Generate a subset from the shuffled dataset\n",
    "subset_data = shuffled_data[['race_progress', 'tyreageprogress', 'is_leader', 'relativecompound', 'racetrackcat', 'fcystatus',\n",
    "        'remaining_pit_stops', 'pursuer_tyre_change', 'close_ahead', 'pitstop']].sample(frac=0.1, random_state=42).copy()\n",
    "\n",
    "# Step 1.2 - Separate input features (X) and target variable (y)\n",
    "X = subset_data[['race_progress', 'tyreageprogress', 'is_leader', 'relativecompound', 'racetrackcat', 'fcystatus',\n",
    "        'remaining_pit_stops', 'pursuer_tyre_change', 'close_ahead']].copy()\n",
    "y = subset_data['pitstop'].copy()\n",
    "\n",
    "# Step 1.3 - Separate categorical and numerical features\n",
    "cat_features = ['is_leader', 'relativecompound', 'racetrackcat', 'fcystatus', 'remaining_pit_stops',\n",
    "                'pursuer_tyre_change', 'close_ahead']\n",
    "num_features = ['race_progress', 'tyreageprogress']\n",
    "\n",
    "# Perform preprocessing on numerical features\n",
    "scaler = StandardScaler()\n",
    "X[num_features] = scaler.fit_transform(X[num_features])\n",
    "\n",
    "# Perform preprocessing on categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X[cat_features])\n",
    "\n",
    "# Combine preprocessed numerical and categorical features\n",
    "X_processed = np.concatenate((X_encoded, X[num_features]), axis=1)\n",
    "\n",
    "# Encode the categorical labels into integer values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Calculate class frequencies\n",
    "class_frequencies = np.bincount(y_encoded)\n",
    "total_samples = np.sum(class_frequencies)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = total_samples / (len(class_frequencies) * class_frequencies)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "# Split the subset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y_encoded, test_size=0.1, random_state=42)\n",
    "\n",
    "# Reshape X_processed to include the timestep dimension\n",
    "X_processed_reshaped = np.reshape(X_processed, (X_processed.shape[0], X_processed.shape[1], 1))\n",
    "\n",
    "# Build the FFNN model with L1 and L2 regularization\n",
    "ffnn_model = tf.keras.models.Sequential()\n",
    "ffnn_model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(X_processed.shape[1],),\n",
    "                                     kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.001)))\n",
    "ffnn_model.add(tf.keras.layers.Dropout(0.2))\n",
    "ffnn_model.add(tf.keras.layers.Dense(64, activation='relu',\n",
    "                                     kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.001)))\n",
    "ffnn_model.add(tf.keras.layers.Dropout(0.2))\n",
    "ffnn_model.add(tf.keras.layers.Dense(64, activation='relu',\n",
    "                                     kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.001)))\n",
    "ffnn_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Build the RNN model\n",
    "rnn_model = tf.keras.models.Sequential()\n",
    "rnn_model.add(tf.keras.layers.SimpleRNN(64, input_shape=(X_processed.shape[1], 1),\n",
    "                                        kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.001)))\n",
    "rnn_model.add(tf.keras.layers.Dense(32, activation='relu',\n",
    "                                    kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.001)))\n",
    "rnn_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Combine the FFNN and RNN models\n",
    "combined_model_input = Input(shape=(X_processed.shape[1],))\n",
    "ffnn_output = ffnn_model(combined_model_input)\n",
    "rnn_input = Reshape((X_processed.shape[1], 1))(combined_model_input)\n",
    "rnn_output = rnn_model(rnn_input)\n",
    "combined_output = concatenate([ffnn_output, rnn_output])\n",
    "combined_output = Dense(1, activation='sigmoid',\n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.001))(combined_output)\n",
    "\n",
    "# Create the combined model\n",
    "combined_model = Model(inputs=combined_model_input, outputs=combined_output)\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = History()\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Create an instance of RandomUnderSampler\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Apply undersampling to X_train and y_train\n",
    "X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train the model using the resampled data\n",
    "history = combined_model.fit(X_train_resampled, y_train_resampled,\n",
    "                             class_weight=class_weights_dict,\n",
    "                             batch_size=256,\n",
    "                             epochs=10,\n",
    "                             validation_split=0.1,\n",
    "                             callbacks=[early_stopping, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c28fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = combined_model.predict(X_val)\n",
    "y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Convert y_val and y_pred to 1-dimensional arrays\n",
    "y_val = y_val.ravel()\n",
    "y_pred = y_pred.ravel()\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_val, y_pred, zero_division=1)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b44ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "p = precision_score(y_val, y_pred)\n",
    "r = recall_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "print(\"Precision:\", p)\n",
    "print(\"Recall\", r)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283e7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision, recall, _ = precision_recall_curve(y_val, y_pred, pos_label=1)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(recall, precision, color='b', label='Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b0fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the whole dataset using the same transformations\n",
    "X_processed = np.concatenate((X_encoded, X[num_features]), axis=1)\n",
    "X_reshaped = np.reshape(X_processed, (X_processed.shape[0], X_processed.shape[1], 1))\n",
    "\n",
    "# Generate predictions on the whole dataset\n",
    "y_pred = np.round(hybrid_model.predict(X_reshaped)).astype(int)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf = confusion_matrix(y_encoded, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf630c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and validation loss values from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Get the predicted values on the validation set\n",
    "y_pred = combined_model.predict(X_val)\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6b45b",
   "metadata": {},
   "source": [
    "## tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, History\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Reshape, SimpleRNN, Flatten, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "\n",
    "# Step 1 - Generate a subset from the shuffled dataset\n",
    "subset_data = shuffled_data[['race_progress', 'tyreageprogress', 'is_leader', 'relativecompound', 'racetrackcat', 'fcystatus',\n",
    "        'remaining_pit_stops', 'pursuer_tyre_change', 'close_ahead', 'pitstop']].sample(frac=0.1, random_state=42).copy()\n",
    "\n",
    "# Step 1.2 - Separate input features (X) and target variable (y)\n",
    "X = subset_data[['race_progress', 'tyreageprogress', 'is_leader', 'relativecompound', 'racetrackcat', 'fcystatus',\n",
    "        'remaining_pit_stops', 'pursuer_tyre_change', 'close_ahead']].copy()\n",
    "y = subset_data['pitstop'].copy()\n",
    "\n",
    "# Step 1.3 - Separate categorical and numerical features\n",
    "cat_features = ['is_leader', 'relativecompound', 'racetrackcat', 'fcystatus', 'remaining_pit_stops',\n",
    "                'pursuer_tyre_change', 'close_ahead']\n",
    "num_features = ['race_progress', 'tyreageprogress']\n",
    "\n",
    "# Perform preprocessing on numerical features\n",
    "scaler = StandardScaler()\n",
    "X[num_features] = scaler.fit_transform(X[num_features])\n",
    "\n",
    "# Perform preprocessing on categorical features\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X[cat_features])\n",
    "\n",
    "# Combine preprocessed numerical and categorical features\n",
    "X_processed = np.concatenate((X_encoded, X[num_features]), axis=1)\n",
    "\n",
    "# Encode the categorical labels into integer values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Calculate class frequencies\n",
    "class_frequencies = np.bincount(y_encoded)\n",
    "total_samples = np.sum(class_frequencies)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = total_samples / (len(class_frequencies) * class_frequencies)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "# Split the subset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y_encoded, test_size=0.1, random_state=42)\n",
    "\n",
    "# Reshape X_processed to include the timestep dimension\n",
    "X_processed_reshaped = np.reshape(X_processed, (X_processed.shape[0], X_processed.shape[1], 1))\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    l1 = trial.suggest_float('l1', 0.001, 0.1, log=True)\n",
    "    l2 = trial.suggest_float('l2', 0.001, 0.1, log=True)\n",
    "\n",
    "    # Build the FFNN model with L1 and L2 regularization\n",
    "    ffnn_model = tf.keras.models.Sequential()\n",
    "    ffnn_model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(X_processed.shape[1],),\n",
    "                                         kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    ffnn_model.add(tf.keras.layers.Dropout(0.2))\n",
    "    ffnn_model.add(tf.keras.layers.Dense(64, activation='relu',\n",
    "                                         kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    ffnn_model.add(tf.keras.layers.Dropout(0.2))\n",
    "    ffnn_model.add(tf.keras.layers.Dense(64, activation='relu',\n",
    "                                         kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    ffnn_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Build the RNN model\n",
    "    rnn_model = tf.keras.models.Sequential()\n",
    "    rnn_model.add(tf.keras.layers.SimpleRNN(64, input_shape=(X_processed.shape[1], 1),\n",
    "                                            kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    rnn_model.add(tf.keras.layers.Dense(32, activation='relu',\n",
    "                                        kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    rnn_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Combine the FFNN and RNN models\n",
    "    combined_model_input = Input(shape=(X_processed.shape[1],))\n",
    "    ffnn_output = ffnn_model(combined_model_input)\n",
    "    rnn_input = Reshape((X_processed.shape[1], 1))(combined_model_input)\n",
    "    rnn_output = rnn_model(rnn_input)\n",
    "    combined_output = concatenate([ffnn_output, rnn_output])\n",
    "    combined_output = Dense(1, activation='sigmoid',\n",
    "                            kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2))(combined_output)\n",
    "\n",
    "    # Create the combined model\n",
    "    combined_model = Model(inputs=combined_model_input, outputs=combined_output)\n",
    "\n",
    "    # Compile the combined model\n",
    "    combined_model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "\n",
    "    # Define the EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    history = History()\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "    # Create an instance of RandomUnderSampler\n",
    "    under_sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "    # Apply undersampling to X_train and y_train\n",
    "    X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Train the model using the resampled data\n",
    "    combined_model.fit(X_train_resampled, y_train_resampled,\n",
    "                       class_weight=class_weights_dict,\n",
    "                       batch_size=256,\n",
    "                       epochs=10,\n",
    "                       validation_split=0.1,\n",
    "                       callbacks=[early_stopping, history])\n",
    "\n",
    "    # Calculate predictions on the validation set\n",
    "    y_val_pred = combined_model.predict(X_val)\n",
    "    y_val_pred_binary = np.round(y_val_pred)\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1 = f1_score(y_val, y_val_pred_binary)\n",
    "\n",
    "    # Return the F1 score as the objective value for Optuna\n",
    "    return f1\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Run the hyperparameter search\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters and the best validation accuracy\n",
    "print('Best hyperparameters:', study.best_params)\n",
    "print('Best F1 score:', study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f267b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
