{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980fb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read df\n",
    "df = pd.read_csv(r'f1dataset2.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0467d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "shuffled_data = df.sample(frac=1, random_state=42)  # Set random_state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c04d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a subset\n",
    "subset_data = shuffled_data[['race_progress', 'remaining_pit_stops', 'relativecompound', 'location', 'fulfilled_second_compound', 'number_of_available_compounds']].sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Separate input features (X) and target variable (y)\n",
    "X = subset_data[['race_progress', 'remaining_pit_stops', 'location', 'fulfilled_second_compound', 'number_of_available_compounds']]\n",
    "y = subset_data['relativecompound']\n",
    "\n",
    "# Separate categorical and numerical features\n",
    "cat_features = ['remaining_pit_stops', 'location', 'fulfilled_second_compound', 'number_of_available_compounds']\n",
    "num_features = ['race_progress']\n",
    "\n",
    "# Perform preprocessing on numerical features\n",
    "scaler = StandardScaler()\n",
    "X[num_features] = scaler.fit_transform(X[num_features])\n",
    "\n",
    "# Perform preprocessing on categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X[cat_features])\n",
    "\n",
    "# Combine preprocessed numerical and categorical features\n",
    "X_processed = np.concatenate((X_encoded, X[num_features]), axis=1)\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the categorical labels into integer values\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the subset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y_encoded, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    units = trial.suggest_int('units', 16, 256)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'rmsprop', 'nadam'])\n",
    "    l2_reg = trial.suggest_loguniform('l2_reg', 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "\n",
    "    # Define the model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(units, activation='relu', input_shape=(X_processed.shape[1],),\n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    # Define the optimizer\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'nadam':\n",
    "        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=50, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred_binary = np.argmax(y_val_pred, axis=1)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred_binary)\n",
    "\n",
    "    # Return the accuracy as the performance metric\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the study name and storage location\n",
    "study_name = \"NN2\"\n",
    "storage = \"sqlite:///NN2.db\"\n",
    "study = optuna.create_study(direction='maximize', study_name=study_name, storage=storage)\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get the best hyperparameters and their performance\n",
    "best_params = study.best_params\n",
    "best_accuracy = study.best_value\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"Best Accuracy: {best_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from optuna.visualization import plot_optimization_history\n",
    "# # Load the saved study\n",
    "# study_name = \"tc_tuning_2\"\n",
    "# storage = \"sqlite:///tc_tuning2.db\"\n",
    "# study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "\n",
    "# # Plot the optimization history\n",
    "# plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b329549",
   "metadata": {},
   "source": [
    "## using the tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1973a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a subset\n",
    "subset_data = shuffled_data[['race_progress', 'remaining_pit_stops', 'relativecompound', 'location', 'fulfilled_second_compound', 'number_of_available_compounds']].sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Separate input features (X) and target variable (y)\n",
    "X = subset_data[['race_progress', 'remaining_pit_stops', 'location', 'fulfilled_second_compound', 'number_of_available_compounds']]\n",
    "y = subset_data['relativecompound']\n",
    "\n",
    "# Separate categorical and numerical features\n",
    "cat_features = ['remaining_pit_stops', 'location', 'fulfilled_second_compound', 'number_of_available_compounds']\n",
    "num_features = ['race_progress']\n",
    "\n",
    "# Perform preprocessing on numerical features\n",
    "scaler = StandardScaler()\n",
    "X[num_features] = scaler.fit_transform(X[num_features])\n",
    "\n",
    "# Perform preprocessing on categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X[cat_features])\n",
    "\n",
    "# Combine preprocessed numerical and categorical features\n",
    "X_processed = np.concatenate((X_encoded, X[num_features]), axis=1)\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the categorical labels into integer values\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the subset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y_encoded, test_size=0.1, random_state=42)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(31, activation='relu', input_shape=(X_processed.shape[1],), kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dropout(0.2553398099761799))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.0075533912129734)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the accuracy values from the training history\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Plot the accuracy graph\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82208494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "_, accuracy = model.evaluate(X_val, y_val)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2caad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the accuracy values from the training history\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Plot the accuracy graph\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
